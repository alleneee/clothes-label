#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆè¡£æœåˆ†ç±»è®­ç»ƒè„šæœ¬
ç›®æ ‡ï¼šå°†å‡†ç¡®ç‡æå‡åˆ°80%+
"""

import os
import sys
import yaml
import argparse
from pathlib import Path
from datetime import datetime

import torch
import pytorch_lightning as pl
from pytorch_lightning.callbacks import (
    ModelCheckpoint, EarlyStopping, LearningRateMonitor,
    TQDMProgressBar, DeviceStatsMonitor, ModelSummary
)
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.strategies import DDPStrategy

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from core.train import ProductClassifier
from core.data_module import HFDatasetsModule


def setup_callbacks(config):
    """è®¾ç½®å›è°ƒå‡½æ•°"""
    callbacks = []
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_config = config['training']['checkpoint']
    
    # ç”ŸæˆåŒ…å«æ—¥æœŸçš„æ–‡ä»¶å
    current_date = datetime.now().strftime("%Y%m%d")
    base_filename = config['checkpointing']['filename']
    # åœ¨åŸºç¡€æ–‡ä»¶åå‰æ·»åŠ æ—¥æœŸ
    filename_with_date = f"{current_date}-{base_filename}"
    
    checkpoint_callback = ModelCheckpoint(
        dirpath=config['checkpointing']['dirpath'],
        filename=filename_with_date,
        monitor=checkpoint_config['monitor'],
        mode=checkpoint_config['mode'],
        save_top_k=checkpoint_config['save_top_k'],
        save_last=checkpoint_config['save_last'],
        every_n_epochs=checkpoint_config.get('every_n_epochs', 1),
        save_weights_only=config['checkpointing'].get('save_weights_only', False),
        auto_insert_metric_name=False
    )
    callbacks.append(checkpoint_callback)
    
    # æ—©åœå›è°ƒ
    early_stopping_config = config['training']['early_stopping']
    early_stopping = EarlyStopping(
        monitor=early_stopping_config['monitor'],
        patience=early_stopping_config['patience'],
        mode=early_stopping_config['mode'],
        min_delta=early_stopping_config['min_delta'],
        verbose=True
    )
    callbacks.append(early_stopping)
    
    # å­¦ä¹ ç‡ç›‘æ§
    lr_monitor = LearningRateMonitor(logging_interval='step')
    callbacks.append(lr_monitor)
    
    # è¿›åº¦æ¡
    progress_bar = TQDMProgressBar()
    callbacks.append(progress_bar)
    
    # è®¾å¤‡çŠ¶æ€ç›‘æ§
    if config['logging'].get('log_gpu_memory', False):
        device_stats = DeviceStatsMonitor()
        callbacks.append(device_stats)
    
    # æ¨¡å‹æ‘˜è¦
    model_summary = ModelSummary(max_depth=2)
    callbacks.append(model_summary)
    
    return callbacks


def setup_logger(config):
    """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
    log_config = config['logging']
    
    logger = TensorBoardLogger(
        save_dir=log_config['log_dir'],
        name=log_config['experiment_name'],
        log_graph=log_config.get('tensorboard', {}).get('log_graph', False),
        default_hp_metric=False
    )
    
    return logger


def setup_trainer(config, callbacks, logger):
    """è®¾ç½®è®­ç»ƒå™¨"""
    training_config = config['training']
    multi_gpu_config = config['multi_gpu']
    
    # åˆ†å¸ƒå¼ç­–ç•¥
    strategy = 'auto'  # é»˜è®¤ä¸ºautoï¼Œè®©PyTorch Lightningè‡ªåŠ¨é€‰æ‹©
    if torch.cuda.device_count() > 1:
        strategy = DDPStrategy(
            find_unused_parameters=multi_gpu_config.get('find_unused_parameters', False)
        )
    
    # è®­ç»ƒå™¨å‚æ•°
    trainer_kwargs = {
        'max_epochs': training_config['max_epochs'],
        'callbacks': callbacks,
        'logger': logger,
        'accelerator': 'gpu' if torch.cuda.is_available() else 'cpu',
        'devices': 'auto',
        'strategy': strategy,
        'precision': multi_gpu_config.get('precision', 32),
        'gradient_clip_val': training_config.get('gradient_clip_val', 0),
        'accumulate_grad_batches': training_config.get('accumulate_grad_batches', 1),
        'deterministic': config.get('performance', {}).get('deterministic', False),
        'benchmark': config.get('performance', {}).get('benchmark', True),
        'enable_progress_bar': True,
        'enable_model_summary': True,
    }
    
    # éªŒè¯é…ç½®
    validation_config = config.get('validation', {})
    trainer_kwargs.update({
        'check_val_every_n_epoch': validation_config.get('check_val_every_n_epoch', 1),
        'val_check_interval': validation_config.get('val_check_interval', 1.0),
        'limit_val_batches': validation_config.get('limit_val_batches', 1.0),
        'num_sanity_val_steps': validation_config.get('num_sanity_val_steps', 2)
    })
    
    # é«˜çº§è®­ç»ƒé…ç½®
    advanced_config = config.get('advanced_training', {})
    adaptive_config = advanced_config.get('adaptive', {})
    
    # æ³¨æ„ï¼šauto_lr_find å’Œ auto_scale_batch_size åœ¨æ–°ç‰ˆæœ¬ä¸­éœ€è¦å•ç‹¬è°ƒç”¨
    # è¿™é‡Œå…ˆåˆ›å»ºtrainerï¼Œç„¶ååœ¨åé¢ä½¿ç”¨tuneræ¥è°ƒç”¨
    
    trainer = pl.Trainer(**trainer_kwargs)
    
    return trainer


def main():
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆè¡£æœåˆ†ç±»è®­ç»ƒ')
    parser.add_argument('--config', type=str, default='config.yaml',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--resume', type=str, default=None,
                        help='ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
    parser.add_argument('--test-only', action='store_true',
                        help='åªè¿›è¡Œæµ‹è¯•')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    print(f"ğŸ“– åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env_vars = config.get('environment_variables', {})
    for key, value in env_vars.items():
        os.environ[key] = str(value)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(config['checkpointing']['dirpath'], exist_ok=True)
    os.makedirs(config['logging']['log_dir'], exist_ok=True)
    
    # è®¾ç½®éšæœºç§å­
    pl.seed_everything(42, workers=True)
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒå‡†å¤‡...")
    
    # 1. æ•°æ®æ¨¡å—
    print("ğŸ“Š åˆå§‹åŒ–æ•°æ®æ¨¡å—...")
    data_module = HFDatasetsModule(
        data_dir=config['data']['data_dir'],
        cache_dir=config['data'].get('cache_dir', 'datasets/hf_cache'),
        dataset_name=config['data'].get('dataset_name', 'clothes_classification_v1'),
        batch_size=config['data']['batch_size'],
        image_size=config['data']['image_size'],
        num_workers=config['data']['num_workers'],
        augmentation_enabled=config['data']['augmentation']['enabled'],
        class_names=None if config['classes'].get('auto_detect', True) else config['classes'].get('names'),
        auto_detect_classes=config['classes'].get('auto_detect', True)
    )
    data_module.prepare_data()
    data_module.setup('fit')
    
    # 2. æ¨¡å‹
    print("ğŸ¤– åˆå§‹åŒ–æ¨¡å‹...")
    if args.resume:
        print(f"ğŸ“¥ ä»æ£€æŸ¥ç‚¹æ¢å¤: {args.resume}")
        model = ProductClassifier.load_from_checkpoint(args.resume, config=config)
    else:
        # è·å–æ•°æ®é›†ä¿¡æ¯
        dataset_info = data_module.get_dataset_info()
        
        # æ›´æ–°é…ç½®ä¸­çš„ç±»åˆ«ä¿¡æ¯
        config['model']['num_classes'] = dataset_info['num_classes']
        config['classes']['names'] = dataset_info['class_names']
        config['classes']['num_classes'] = dataset_info['num_classes']
        
        model = ProductClassifier(config)
        
        # è®¾ç½®ç±»åˆ«æƒé‡
        if data_module.class_weights is not None:
            model.class_weights = data_module.class_weights
    
    # 3. å›è°ƒå’Œæ—¥å¿—
    print("ğŸ“ è®¾ç½®å›è°ƒå’Œæ—¥å¿—...")
    callbacks = setup_callbacks(config)
    logger = setup_logger(config)
    
    # 4. è®­ç»ƒå™¨
    print("âš™ï¸ è®¾ç½®è®­ç»ƒå™¨...")
    trainer = setup_trainer(config, callbacks, logger)
    
    # 5. æ¨¡å‹ä¿¡æ¯
    print("\n" + "="*60)
    print("ğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"   - æ¨¡å‹åç§°: {config['model']['name']}")
    print(f"   - ç±»åˆ«æ•°é‡: {dataset_info['num_classes']}")
    print(f"   - å›¾ç‰‡å°ºå¯¸: {config['data']['image_size']}x{config['data']['image_size']}")
    print(f"   - æ‰¹æ¬¡å¤§å°: {config['data']['batch_size']}")
    print(f"   - å­¦ä¹ ç‡: {config['model']['learning_rate']}")
    print(f"   - æœ€å¤§è½®æ•°: {config['training']['max_epochs']}")
    
    print("\nğŸ“Š æ•°æ®ä¿¡æ¯:")
    print(f"   - è®­ç»ƒæ ·æœ¬: {len(data_module.train_dataset)}")
    if data_module.val_dataset:
        print(f"   - éªŒè¯æ ·æœ¬: {len(data_module.val_dataset)}")
    
    print("\nğŸ“Š å¢å¼ºè®¾ç½®:")
    aug_config = config['data'].get('augmentation', {})
    if aug_config.get('enabled', False):
        print(f"   - æ•°æ®å¢å¼º: å¯ç”¨")
        print(f"   - Mixup Alpha: {aug_config.get('mixup_alpha', 0)}")
        print(f"   - CutMix Alpha: {aug_config.get('cutmix_alpha', 0)}")
        print(f"   - æ—‹è½¬è§’åº¦: {aug_config.get('rotation_degrees', 0)}Â°")
        print(f"   - æ°´å¹³ç¿»è½¬: {aug_config.get('horizontal_flip', 0)}")
    else:
        print(f"   - æ•°æ®å¢å¼º: ç¦ç”¨")
    
    print("\nğŸ“Š è®¾å¤‡ä¿¡æ¯:")
    print(f"   - å¯ç”¨GPU: {torch.cuda.device_count()}")
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            print(f"   - GPU {i}: {gpu_name}")
    print("="*60 + "\n")
    
    # 6. è‡ªåŠ¨è°ƒä¼˜ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    advanced_config = config.get('advanced_training', {})
    adaptive_config = advanced_config.get('adaptive', {})
    
    # æš‚æ—¶ç¦ç”¨è‡ªåŠ¨è°ƒä¼˜åŠŸèƒ½ï¼Œé¿å…ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
    # if adaptive_config.get('lr_finder', False):
    #     print("ğŸ” è‡ªåŠ¨å¯»æ‰¾æœ€ä½³å­¦ä¹ ç‡...")
    #     lr_finder = trainer.tuner.lr_find(model, data_module)
    #     new_lr = lr_finder.suggestion()
    #     print(f"ğŸ’¡ å»ºè®®å­¦ä¹ ç‡: {new_lr}")
    #     model.learning_rate = new_lr
    # 
    # if adaptive_config.get('auto_batch_size', False):
    #     print("ğŸ” è‡ªåŠ¨è°ƒæ•´æ‰¹æ¬¡å¤§å°...")
    #     trainer.tuner.scale_batch_size(model, data_module, mode='power')
    #     print(f"ğŸ’¡ è°ƒæ•´åæ‰¹æ¬¡å¤§å°: {data_module.batch_size}")
    
    try:
        if args.test_only:
            # åªè¿›è¡Œæµ‹è¯•
            print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
            trainer.test(model, data_module)
        else:
            # å¼€å§‹è®­ç»ƒ
            print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
            trainer.fit(model, data_module, ckpt_path=args.resume)
            
            # è®­ç»ƒå®Œæˆåè¿›è¡Œæµ‹è¯•ï¼ˆä½¿ç”¨éªŒè¯é›†ä½œä¸ºæµ‹è¯•é›†ï¼‰
            if hasattr(data_module, 'val_dataset') and data_module.val_dataset:
                print("ğŸ§ª å¼€å§‹æµ‹è¯•...")
                trainer.test(model, data_module, ckpt_path='best')
    
    except KeyboardInterrupt:
        print("\nâš ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("ğŸ è®­ç»ƒç»“æŸ")


if __name__ == "__main__":
    main() 