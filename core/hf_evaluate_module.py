#!/usr/bin/env python3
"""
HuggingFace Evaluate è¯„ä¼°æ¨¡å—
ä½¿ç”¨æ ‡å‡†åŒ–æŒ‡æ ‡è¯„ä¼°è¡£æœåˆ†ç±»æ¨¡å‹æ€§èƒ½
"""

import torch
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
import logging
from pathlib import Path
import json
from datetime import datetime

try:
    import evaluate
    HF_EVALUATE_AVAILABLE = True
except ImportError:
    HF_EVALUATE_AVAILABLE = False
    logging.warning("HuggingFace Evaluate not available. Install with: pip install evaluate")

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class HFEvaluateModule:
    """HuggingFace Evaluate è¯„ä¼°æ¨¡å—"""
    
    def __init__(
        self,
        class_names: List[str],
        metrics_to_compute: Optional[List[str]] = None,
        output_dir: str = "evaluation_results",
        save_results: bool = True
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°æ¨¡å—
        
        Args:
            class_names: ç±»åˆ«åç§°åˆ—è¡¨
            metrics_to_compute: è¦è®¡ç®—çš„æŒ‡æ ‡åˆ—è¡¨
            output_dir: ç»“æœè¾“å‡ºç›®å½•
            save_results: æ˜¯å¦ä¿å­˜ç»“æœ
        """
        self.class_names = class_names
        self.num_classes = len(class_names)
        self.output_dir = Path(output_dir)
        self.save_results = save_results
        
        if self.save_results:
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # é»˜è®¤æŒ‡æ ‡
        if metrics_to_compute is None:
            metrics_to_compute = [
                "accuracy",
                "precision", 
                "recall", 
                "f1",
                "matthews_correlation"
            ]
        
        self.metrics_to_compute = metrics_to_compute
        
        # åˆå§‹åŒ–HF EvaluateæŒ‡æ ‡
        self.hf_metrics = {}
        if HF_EVALUATE_AVAILABLE:
            self._initialize_hf_metrics()
        else:
            logger.warning("ä½¿ç”¨fallbackè¯„ä¼°æ–¹æ³•")
    
    def _initialize_hf_metrics(self):
        """åˆå§‹åŒ–HuggingFace EvaluateæŒ‡æ ‡"""
        try:
            for metric_name in self.metrics_to_compute:
                if metric_name in ["precision", "recall", "f1"]:
                    # è¿™äº›æŒ‡æ ‡éœ€è¦æŒ‡å®šå¹³å‡æ–¹å¼
                    self.hf_metrics[metric_name] = evaluate.load(metric_name)
                else:
                    self.hf_metrics[metric_name] = evaluate.load(metric_name)
            
            logger.info(f"âœ… æˆåŠŸåˆå§‹åŒ–HF EvaluateæŒ‡æ ‡: {list(self.hf_metrics.keys())}")
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–HF EvaluateæŒ‡æ ‡å¤±è´¥: {e}")
            HF_EVALUATE_AVAILABLE = False
    
    def compute_metrics(
        self,
        predictions: np.ndarray,
        references: np.ndarray,
        probabilities: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Args:
            predictions: é¢„æµ‹ç»“æœ
            references: çœŸå®æ ‡ç­¾
            probabilities: é¢„æµ‹æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è®¡ç®—å¾—åˆ°çš„æŒ‡æ ‡å­—å…¸
        """
        if HF_EVALUATE_AVAILABLE:
            return self._compute_hf_metrics(predictions, references, probabilities)
        else:
            return self._compute_fallback_metrics(predictions, references, probabilities)
    
    def _compute_hf_metrics(
        self,
        predictions: np.ndarray,
        references: np.ndarray,
        probabilities: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨HuggingFace Evaluateè®¡ç®—æŒ‡æ ‡"""
        results = {}
        
        for metric_name, metric in self.hf_metrics.items():
            try:
                if metric_name == "accuracy":
                    result = metric.compute(predictions=predictions, references=references)
                    results[metric_name] = result['accuracy']
                
                elif metric_name in ["precision", "recall", "f1"]:
                    # è®¡ç®—macroå’Œmicroå¹³å‡
                    for avg_type in ["macro", "micro"]:
                        result = metric.compute(
                            predictions=predictions, 
                            references=references,
                            average=avg_type
                        )
                        results[f"{metric_name}_{avg_type}"] = result[metric_name]
                    
                    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
                    result_per_class = metric.compute(
                        predictions=predictions, 
                        references=references,
                        average=None
                    )
                    for i, class_name in enumerate(self.class_names):
                        if i < len(result_per_class[metric_name]):
                            results[f"{metric_name}_{class_name}"] = result_per_class[metric_name][i]
                
                elif metric_name == "matthews_correlation":
                    result = metric.compute(predictions=predictions, references=references)
                    results[metric_name] = result['matthews_correlation']
                
                else:
                    # é€šç”¨æŒ‡æ ‡è®¡ç®—
                    result = metric.compute(predictions=predictions, references=references)
                    results[metric_name] = result.get(metric_name, result)
                    
            except Exception as e:
                logger.error(f"âŒ è®¡ç®—æŒ‡æ ‡ {metric_name} å¤±è´¥: {e}")
                continue
        
        # æ·»åŠ é¢å¤–çš„è‡ªå®šä¹‰æŒ‡æ ‡
        results.update(self._compute_custom_metrics(predictions, references, probabilities))
        
        return results
    
    def _compute_fallback_metrics(
        self,
        predictions: np.ndarray,
        references: np.ndarray,
        probabilities: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """ä½¿ç”¨sklearnä½œä¸ºfallbackè®¡ç®—æŒ‡æ ‡"""
        from sklearn.metrics import (
            accuracy_score, precision_score, recall_score, f1_score,
            classification_report, matthews_corrcoef
        )
        
        results = {}
        
        try:
            # åŸºç¡€æŒ‡æ ‡
            results['accuracy'] = accuracy_score(references, predictions)
            results['precision_macro'] = precision_score(references, predictions, average='macro', zero_division=0)
            results['precision_micro'] = precision_score(references, predictions, average='micro', zero_division=0)
            results['recall_macro'] = recall_score(references, predictions, average='macro', zero_division=0)
            results['recall_micro'] = recall_score(references, predictions, average='micro', zero_division=0)
            results['f1_macro'] = f1_score(references, predictions, average='macro', zero_division=0)
            results['f1_micro'] = f1_score(references, predictions, average='micro', zero_division=0)
            results['matthews_correlation'] = matthews_corrcoef(references, predictions)
            
            # æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
            precision_per_class = precision_score(references, predictions, average=None, zero_division=0)
            recall_per_class = recall_score(references, predictions, average=None, zero_division=0)
            f1_per_class = f1_score(references, predictions, average=None, zero_division=0)
            
            for i, class_name in enumerate(self.class_names):
                if i < len(precision_per_class):
                    results[f'precision_{class_name}'] = precision_per_class[i]
                    results[f'recall_{class_name}'] = recall_per_class[i]
                    results[f'f1_{class_name}'] = f1_per_class[i]
            
            # æ·»åŠ é¢å¤–çš„è‡ªå®šä¹‰æŒ‡æ ‡
            results.update(self._compute_custom_metrics(predictions, references, probabilities))
            
        except Exception as e:
            logger.error(f"âŒ FallbackæŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
            results['accuracy'] = accuracy_score(references, predictions)
        
        return results
    
    def _compute_custom_metrics(
        self,
        predictions: np.ndarray,
        references: np.ndarray,
        probabilities: Optional[np.ndarray] = None
    ) -> Dict[str, Any]:
        """è®¡ç®—è‡ªå®šä¹‰æŒ‡æ ‡"""
        custom_metrics = {}
        
        # æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
        for i, class_name in enumerate(self.class_names):
            class_mask = references == i
            if np.sum(class_mask) > 0:
                class_acc = np.sum(predictions[class_mask] == references[class_mask]) / np.sum(class_mask)
                custom_metrics[f'accuracy_{class_name}'] = class_acc
        
        # é’ˆå¯¹èº«ä½“åˆ†ç±»çš„ç‰¹æ®ŠæŒ‡æ ‡
        if any('å…¨èº«' in name for name in self.class_names) and any('åŠèº«' in name for name in self.class_names):
            custom_metrics.update(self._compute_body_classification_metrics(predictions, references))
        
        return custom_metrics
    
    def _compute_body_classification_metrics(
        self,
        predictions: np.ndarray,
        references: np.ndarray
    ) -> Dict[str, Any]:
        """è®¡ç®—èº«ä½“åˆ†ç±»ç‰¹æ®ŠæŒ‡æ ‡"""
        metrics = {}
        
        # æ‰¾åˆ°å…¨èº«å’ŒåŠèº«ç±»åˆ«çš„ç´¢å¼•
        full_body_idx = None
        half_body_idx = None
        
        for i, class_name in enumerate(self.class_names):
            if 'å…¨èº«' in class_name:
                full_body_idx = i
            elif 'åŠèº«' in class_name:
                half_body_idx = i
        
        if full_body_idx is not None and half_body_idx is not None:
            # å…¨èº«æ¨¡ç‰¹è¯¯åˆ†ç±»ä¸ºåŠèº«çš„æ¯”ä¾‹
            full_body_mask = references == full_body_idx
            if np.sum(full_body_mask) > 0:
                full_to_half_errors = np.sum(predictions[full_body_mask] == half_body_idx)
                metrics['full_body_to_half_body_error_rate'] = full_to_half_errors / np.sum(full_body_mask)
            
            # åŠèº«æ¨¡ç‰¹è¯¯åˆ†ç±»ä¸ºå…¨èº«çš„æ¯”ä¾‹
            half_body_mask = references == half_body_idx
            if np.sum(half_body_mask) > 0:
                half_to_full_errors = np.sum(predictions[half_body_mask] == full_body_idx)
                metrics['half_body_to_full_body_error_rate'] = half_to_full_errors / np.sum(half_body_mask)
        
        return metrics
    
    def plot_confusion_matrix(
        self,
        predictions: np.ndarray,
        references: np.ndarray,
        save_path: Optional[str] = None
    ) -> plt.Figure:
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        cm = confusion_matrix(references, predictions)
        
        # åˆ›å»ºå›¾åƒ
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        sns.heatmap(
            cm,
            annot=True,
            fmt='d',
            cmap='Blues',
            xticklabels=self.class_names,
            yticklabels=self.class_names,
            ax=ax
        )
        
        ax.set_title('Confusion Matrix', fontsize=16, pad=20)
        ax.set_xlabel('Predicted Label', fontsize=12)
        ax.set_ylabel('True Label', fontsize=12)
        
        # æ—‹è½¬æ ‡ç­¾ä»¥é¿å…é‡å 
        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')
        plt.setp(ax.get_yticklabels(), rotation=0)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        
        return fig
    
    def generate_evaluation_report(
        self,
        predictions: np.ndarray,
        references: np.ndarray,
        probabilities: Optional[np.ndarray] = None,
        dataset_name: str = "test"
    ) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Š"""
        logger.info(f"ğŸ“Š ç”Ÿæˆ {dataset_name} é›†è¯„ä¼°æŠ¥å‘Š...")
        
        # è®¡ç®—æŒ‡æ ‡
        metrics = self.compute_metrics(predictions, references, probabilities)
        
        # åˆ›å»ºæŠ¥å‘Š
        report = {
            'dataset': dataset_name,
            'timestamp': datetime.now().isoformat(),
            'num_samples': len(predictions),
            'num_classes': self.num_classes,
            'class_names': self.class_names,
            'metrics': metrics
        }
        
        # ä¿å­˜ç»“æœ
        if self.save_results:
            # ä¿å­˜æŒ‡æ ‡
            metrics_file = self.output_dir / f"{dataset_name}_metrics.json"
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜æ··æ·†çŸ©é˜µ
            cm_file = self.output_dir / f"{dataset_name}_confusion_matrix.png"
            self.plot_confusion_matrix(predictions, references, str(cm_file))
            
            logger.info(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° {self.output_dir}")
        
        return report
    
    def print_evaluation_summary(self, report: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print(f"ğŸ“Š {report['dataset'].upper()} é›†è¯„ä¼°æŠ¥å‘Š")
        print("="*60)
        print(f"æ ·æœ¬æ•°é‡: {report['num_samples']}")
        print(f"ç±»åˆ«æ•°é‡: {report['num_classes']}")
        print("-"*60)
        
        metrics = report['metrics']
        
        # ä¸»è¦æŒ‡æ ‡
        print("ğŸ“ˆ ä¸»è¦æŒ‡æ ‡:")
        main_metrics = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro', 'matthews_correlation']
        for metric in main_metrics:
            if metric in metrics:
                print(f"  {metric:20}: {metrics[metric]:.4f}")
        
        # èº«ä½“åˆ†ç±»ç‰¹æ®ŠæŒ‡æ ‡
        body_metrics = [k for k in metrics.keys() if 'body' in k and 'error_rate' in k]
        if body_metrics:
            print("\nğŸ¯ èº«ä½“åˆ†ç±»é”™è¯¯ç‡:")
            for metric in body_metrics:
                print(f"  {metric:35}: {metrics[metric]:.4f}")
        
        # æ¯ä¸ªç±»åˆ«çš„F1å¾—åˆ†
        print("\nğŸ“Š å„ç±»åˆ«F1å¾—åˆ†:")
        for class_name in self.class_names:
            f1_key = f'f1_{class_name}'
            if f1_key in metrics:
                print(f"  {class_name:20}: {metrics[f1_key]:.4f}")
        
        print("="*60)


def create_hf_evaluator(class_names: List[str], **kwargs) -> HFEvaluateModule:
    """åˆ›å»ºHFè¯„ä¼°å™¨çš„å·¥å‚å‡½æ•°"""
    return HFEvaluateModule(class_names, **kwargs)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    class_names = [
        "logo", "ä¸‹æ‘†", "ä¾§é¢", "å…¶ä»–", "å£è¢‹", "æ­£é¢", 
        "æ­£é¢å…¨èº«æ¨¡ç‰¹", "æ­£é¢åŠèº«æ¨¡ç‰¹", "èƒŒé¢", "èƒŒé¢æ¨¡ç‰¹", "è¢–å£", "é¢†å£"
    ]
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = HFEvaluateModule(class_names)
    
    # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
    np.random.seed(42)
    predictions = np.random.randint(0, len(class_names), 100)
    references = np.random.randint(0, len(class_names), 100)
    
    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
    report = evaluator.generate_evaluation_report(predictions, references)
    evaluator.print_evaluation_summary(report)
