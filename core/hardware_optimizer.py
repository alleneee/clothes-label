#!/usr/bin/env python3
"""
ç¡¬ä»¶é…ç½®ä¼˜åŒ–å™¨
æ ¹æ®æœºå™¨ç¡¬ä»¶é…ç½®è‡ªåŠ¨è°ƒæ•´è®­ç»ƒå‚æ•°
"""

import psutil
import platform
import subprocess
import json
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import yaml


class HardwareDetector:
    """ç¡¬ä»¶æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.hardware_info = {}
        self.detect_hardware()
    
    def detect_hardware(self):
        """æ£€æµ‹ç¡¬ä»¶é…ç½®"""
        print("æ£€æµ‹ç¡¬ä»¶é…ç½®...")
        
        # åŸºæœ¬ç³»ç»Ÿä¿¡æ¯
        self.hardware_info['system'] = {
            'platform': platform.system(),
            'architecture': platform.machine(),
            'processor': platform.processor(),
            'python_version': platform.python_version()
        }
        
        # CPUä¿¡æ¯
        self.hardware_info['cpu'] = {
            'physical_cores': psutil.cpu_count(logical=False),
            'logical_cores': psutil.cpu_count(logical=True),
            'max_frequency': psutil.cpu_freq().max if psutil.cpu_freq() else None,
            'current_frequency': psutil.cpu_freq().current if psutil.cpu_freq() else None
        }
        
        # å†…å­˜ä¿¡æ¯
        memory = psutil.virtual_memory()
        self.hardware_info['memory'] = {
            'total_gb': round(memory.total / (1024**3), 2),
            'available_gb': round(memory.available / (1024**3), 2),
            'percent_used': memory.percent
        }
        
        # GPUä¿¡æ¯
        self.hardware_info['gpu'] = self.detect_gpu()
        
        # å­˜å‚¨ä¿¡æ¯
        self.hardware_info['storage'] = self.detect_storage()
    
    def detect_gpu(self) -> Dict[str, Any]:
        """æ£€æµ‹GPUä¿¡æ¯"""
        gpu_info = {
            'has_cuda': False,
            'has_mps': False,
            'gpu_count': 0,
            'gpu_memory_gb': 0,
            'gpu_names': [],
            'cuda_version': None
        }
        
        try:
            import torch
            
            # æ£€æµ‹CUDA
            if torch.cuda.is_available():
                gpu_info['has_cuda'] = True
                gpu_info['gpu_count'] = torch.cuda.device_count()
                gpu_info['cuda_version'] = torch.version.cuda
                
                for i in range(gpu_info['gpu_count']):
                    gpu_name = torch.cuda.get_device_name(i)
                    gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                    gpu_info['gpu_names'].append(gpu_name)
                    gpu_info['gpu_memory_gb'] += gpu_memory
                
                gpu_info['gpu_memory_gb'] = round(gpu_info['gpu_memory_gb'], 2)
            
            # æ£€æµ‹MPS (Apple Silicon)
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                gpu_info['has_mps'] = True
                gpu_info['gpu_count'] = 1
                gpu_info['gpu_names'] = ['Apple Silicon GPU']
                # MPSå†…å­˜é€šå¸¸ä¸ç³»ç»Ÿå†…å­˜å…±äº«
                gpu_info['gpu_memory_gb'] = self.hardware_info['memory']['total_gb']
        
        except ImportError:
            print("âš ï¸  PyTorchæœªå®‰è£…ï¼Œæ— æ³•æ£€æµ‹GPUä¿¡æ¯")
        
        return gpu_info
    
    def detect_storage(self) -> Dict[str, Any]:
        """æ£€æµ‹å­˜å‚¨ä¿¡æ¯"""
        try:
            disk_usage = psutil.disk_usage('/')
            return {
                'total_gb': round(disk_usage.total / (1024**3), 2),
                'free_gb': round(disk_usage.free / (1024**3), 2),
                'used_percent': round((disk_usage.used / disk_usage.total) * 100, 2)
            }
        except:
            return {'total_gb': 0, 'free_gb': 0, 'used_percent': 0}
    
    def get_hardware_tier(self) -> str:
        """è·å–ç¡¬ä»¶ç­‰çº§"""
        gpu_memory = self.hardware_info['gpu']['gpu_memory_gb']
        system_memory = self.hardware_info['memory']['total_gb']
        cpu_cores = self.hardware_info['cpu']['logical_cores']
        
        # é«˜ç«¯é…ç½®
        if gpu_memory >= 16 and system_memory >= 32 and cpu_cores >= 16:
            return 'high_end'
        # ä¸­é«˜ç«¯é…ç½®
        elif gpu_memory >= 8 and system_memory >= 16 and cpu_cores >= 8:
            return 'mid_high'
        # ä¸­ç«¯é…ç½®
        elif gpu_memory >= 4 and system_memory >= 8 and cpu_cores >= 4:
            return 'mid_range'
        # ä½ç«¯é…ç½®
        else:
            return 'low_end'
    
    def print_hardware_info(self):
        """æ‰“å°ç¡¬ä»¶ä¿¡æ¯"""
        print("="*60)
        print("ç¡¬ä»¶é…ç½®ä¿¡æ¯")
        print("="*60)
        
        # ç³»ç»Ÿä¿¡æ¯
        sys_info = self.hardware_info['system']
        print(f"æ“ä½œç³»ç»Ÿ: {sys_info['platform']}")
        print(f"æ¶æ„: {sys_info['architecture']}")
        print(f"Pythonç‰ˆæœ¬: {sys_info['python_version']}")
        
        # CPUä¿¡æ¯
        cpu_info = self.hardware_info['cpu']
        print(f"\nCPU:")
        print(f"  ç‰©ç†æ ¸å¿ƒ: {cpu_info['physical_cores']}")
        print(f"  é€»è¾‘æ ¸å¿ƒ: {cpu_info['logical_cores']}")
        if cpu_info['max_frequency']:
            print(f"  æœ€å¤§é¢‘ç‡: {cpu_info['max_frequency']:.0f} MHz")
        
        # å†…å­˜ä¿¡æ¯
        mem_info = self.hardware_info['memory']
        print(f"\nå†…å­˜:")
        print(f"  æ€»å†…å­˜: {mem_info['total_gb']} GB")
        print(f"  å¯ç”¨å†…å­˜: {mem_info['available_gb']} GB")
        print(f"  ä½¿ç”¨ç‡: {mem_info['percent_used']:.1f}%")
        
        # GPUä¿¡æ¯
        gpu_info = self.hardware_info['gpu']
        print(f"\nGPU:")
        if gpu_info['has_cuda']:
            print(f"  CUDAå¯ç”¨: âœ…")
            print(f"  CUDAç‰ˆæœ¬: {gpu_info['cuda_version']}")
            print(f"  GPUæ•°é‡: {gpu_info['gpu_count']}")
            print(f"  GPUå†…å­˜: {gpu_info['gpu_memory_gb']} GB")
            for i, name in enumerate(gpu_info['gpu_names']):
                print(f"  GPU {i}: {name}")
        elif gpu_info['has_mps']:
            print(f"  MPSå¯ç”¨: âœ… (Apple Silicon)")
            print(f"  GPUå†…å­˜: ä¸ç³»ç»Ÿå†…å­˜å…±äº«")
        else:
            print(f"  GPUåŠ é€Ÿ: âŒ (ä»…CPU)")
        
        # å­˜å‚¨ä¿¡æ¯
        storage_info = self.hardware_info['storage']
        print(f"\nå­˜å‚¨:")
        print(f"  æ€»å®¹é‡: {storage_info['total_gb']} GB")
        print(f"  å¯ç”¨ç©ºé—´: {storage_info['free_gb']} GB")
        print(f"  ä½¿ç”¨ç‡: {storage_info['used_percent']:.1f}%")
        
        # ç¡¬ä»¶ç­‰çº§
        tier = self.get_hardware_tier()
        tier_names = {
            'high_end': 'é«˜ç«¯',
            'mid_high': 'ä¸­é«˜ç«¯', 
            'mid_range': 'ä¸­ç«¯',
            'low_end': 'ä½ç«¯'
        }
        print(f"\nç¡¬ä»¶ç­‰çº§: {tier_names.get(tier, tier)}")


class ConfigOptimizer:
    """é…ç½®ä¼˜åŒ–å™¨"""
    
    def __init__(self, hardware_detector: HardwareDetector):
        self.hardware = hardware_detector
        self.tier = hardware_detector.get_hardware_tier()
    
    def optimize_training_config(self, base_config: Dict[str, Any]) -> Dict[str, Any]:
        """ä¼˜åŒ–è®­ç»ƒé…ç½®"""
        optimized_config = base_config.copy()
        
        # æ ¹æ®ç¡¬ä»¶ç­‰çº§è°ƒæ•´å‚æ•°
        if self.tier == 'high_end':
            optimized_config = self._optimize_high_end(optimized_config)
        elif self.tier == 'mid_high':
            optimized_config = self._optimize_mid_high(optimized_config)
        elif self.tier == 'mid_range':
            optimized_config = self._optimize_mid_range(optimized_config)
        else:  # low_end
            optimized_config = self._optimize_low_end(optimized_config)
        
        # é€šç”¨ä¼˜åŒ–
        optimized_config = self._apply_common_optimizations(optimized_config)
        
        return optimized_config
    
    def _optimize_high_end(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """é«˜ç«¯ç¡¬ä»¶ä¼˜åŒ–"""
        # æ•°æ®é…ç½®
        config['data']['batch_size'] = min(64, config['data'].get('batch_size', 32) * 2)
        config['data']['num_workers'] = min(16, self.hardware.hardware_info['cpu']['logical_cores'])
        
        # è®­ç»ƒé…ç½®
        config['training']['mixed_precision'] = True
        config['training']['gradient_clip_val'] = 1.0
        
        # Fine-tuneé…ç½®
        if 'finetune' in config:
            config['finetune']['training']['max_epochs'] = min(30, 
                config['finetune']['training'].get('max_epochs', 20))
        
        return config
    
    def _optimize_mid_high(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸­é«˜ç«¯ç¡¬ä»¶ä¼˜åŒ–"""
        # æ•°æ®é…ç½®
        config['data']['batch_size'] = min(48, config['data'].get('batch_size', 32) + 16)
        config['data']['num_workers'] = min(12, self.hardware.hardware_info['cpu']['logical_cores'])
        
        # è®­ç»ƒé…ç½®
        config['training']['mixed_precision'] = True
        config['training']['gradient_clip_val'] = 1.0
        
        return config
    
    def _optimize_mid_range(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ä¸­ç«¯ç¡¬ä»¶ä¼˜åŒ–"""
        # æ•°æ®é…ç½®
        config['data']['batch_size'] = min(32, config['data'].get('batch_size', 32))
        config['data']['num_workers'] = min(8, self.hardware.hardware_info['cpu']['logical_cores'])
        
        # è®­ç»ƒé…ç½®
        config['training']['mixed_precision'] = True
        config['training']['gradient_clip_val'] = 0.5
        
        return config
    
    def _optimize_low_end(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """ä½ç«¯ç¡¬ä»¶ä¼˜åŒ–"""
        # æ•°æ®é…ç½®
        config['data']['batch_size'] = min(16, config['data'].get('batch_size', 32))
        config['data']['num_workers'] = min(4, max(1, self.hardware.hardware_info['cpu']['logical_cores'] // 2))
        
        # è®­ç»ƒé…ç½®
        config['training']['mixed_precision'] = False  # å¯èƒ½ä¸æ”¯æŒ
        config['training']['gradient_clip_val'] = 0.5
        config['training']['max_epochs'] = min(50, config['training'].get('max_epochs', 100))
        
        # Fine-tuneé…ç½®
        if 'finetune' in config:
            config['finetune']['training']['max_epochs'] = min(15, 
                config['finetune']['training'].get('max_epochs', 20))
            config['finetune']['training']['learning_rate'] *= 0.5  # é™ä½å­¦ä¹ ç‡
        
        return config
    
    def _apply_common_optimizations(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨é€šç”¨ä¼˜åŒ–"""
        # æ ¹æ®GPUå†…å­˜è°ƒæ•´æ‰¹æ¬¡å¤§å°
        gpu_memory = self.hardware.hardware_info['gpu']['gpu_memory_gb']
        
        if gpu_memory > 0:
            # æ ¹æ®GPUå†…å­˜è¿›ä¸€æ­¥è°ƒæ•´æ‰¹æ¬¡å¤§å°
            if gpu_memory < 4:
                config['data']['batch_size'] = min(config['data']['batch_size'], 8)
            elif gpu_memory < 8:
                config['data']['batch_size'] = min(config['data']['batch_size'], 16)
            elif gpu_memory < 12:
                config['data']['batch_size'] = min(config['data']['batch_size'], 32)
        else:
            # ä»…CPUï¼Œå¤§å¹…é™ä½æ‰¹æ¬¡å¤§å°
            config['data']['batch_size'] = min(config['data']['batch_size'], 4)
            config['training']['mixed_precision'] = False
        
        # ç¡®ä¿num_workersä¸è¶…è¿‡CPUæ ¸å¿ƒæ•°
        max_workers = max(1, self.hardware.hardware_info['cpu']['logical_cores'] - 1)
        config['data']['num_workers'] = min(config['data']['num_workers'], max_workers)
        
        return config
    
    def generate_optimization_report(self, original_config: Dict[str, Any], 
                                   optimized_config: Dict[str, Any]) -> str:
        """ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š"""
        report = []
        report.append("="*60)
        report.append("é…ç½®ä¼˜åŒ–æŠ¥å‘Š")
        report.append("="*60)
        
        report.append(f"ç¡¬ä»¶ç­‰çº§: {self.tier}")
        report.append(f"GPUå†…å­˜: {self.hardware.hardware_info['gpu']['gpu_memory_gb']} GB")
        report.append(f"ç³»ç»Ÿå†…å­˜: {self.hardware.hardware_info['memory']['total_gb']} GB")
        report.append(f"CPUæ ¸å¿ƒ: {self.hardware.hardware_info['cpu']['logical_cores']}")
        
        report.append("\nå‚æ•°è°ƒæ•´:")
        
        # æ¯”è¾ƒå…³é”®å‚æ•°
        key_params = [
            ('data.batch_size', 'æ‰¹æ¬¡å¤§å°'),
            ('data.num_workers', 'æ•°æ®åŠ è½½çº¿ç¨‹'),
            ('training.mixed_precision', 'æ··åˆç²¾åº¦'),
            ('training.max_epochs', 'æœ€å¤§è½®æ•°'),
            ('training.gradient_clip_val', 'æ¢¯åº¦è£å‰ª')
        ]
        
        for param_path, param_name in key_params:
            original_val = self._get_nested_value(original_config, param_path)
            optimized_val = self._get_nested_value(optimized_config, param_path)
            
            if original_val != optimized_val:
                report.append(f"  {param_name}: {original_val} â†’ {optimized_val}")
        
        # Fine-tuneå‚æ•°
        if 'finetune' in optimized_config:
            ft_params = [
                ('finetune.training.max_epochs', 'Fine-tuneè½®æ•°'),
                ('finetune.training.learning_rate', 'Fine-tuneå­¦ä¹ ç‡')
            ]
            
            for param_path, param_name in ft_params:
                original_val = self._get_nested_value(original_config, param_path)
                optimized_val = self._get_nested_value(optimized_config, param_path)
                
                if original_val != optimized_val:
                    report.append(f"  {param_name}: {original_val} â†’ {optimized_val}")
        
        return "\n".join(report)
    
    def _get_nested_value(self, config: Dict[str, Any], path: str) -> Any:
        """è·å–åµŒå¥—å­—å…¸çš„å€¼"""
        keys = path.split('.')
        value = config
        
        try:
            for key in keys:
                value = value[key]
            return value
        except (KeyError, TypeError):
            return None


def load_config(config_path: str = 'config.yaml') -> Dict[str, Any]:
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def save_config(config: Dict[str, Any], config_path: str):
    """ä¿å­˜é…ç½®æ–‡ä»¶"""
    with open(config_path, 'w', encoding='utf-8') as f:
        yaml.dump(config, f, default_flow_style=False, allow_unicode=True, indent=2)


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ç¡¬ä»¶é…ç½®ä¼˜åŒ–å™¨')
    parser.add_argument('--config', type=str, default='config.yaml', 
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, 
                       help='ä¼˜åŒ–åé…ç½®æ–‡ä»¶è¾“å‡ºè·¯å¾„')
    parser.add_argument('--detect-only', action='store_true', 
                       help='ä»…æ£€æµ‹ç¡¬ä»¶ï¼Œä¸ä¼˜åŒ–é…ç½®')
    parser.add_argument('--save-hardware-info', action='store_true', 
                       help='ä¿å­˜ç¡¬ä»¶ä¿¡æ¯åˆ°æ–‡ä»¶')
    
    args = parser.parse_args()
    
    # æ£€æµ‹ç¡¬ä»¶
    print("ç¡¬ä»¶é…ç½®ä¼˜åŒ–å™¨")
    detector = HardwareDetector()
    detector.print_hardware_info()
    
    # ä¿å­˜ç¡¬ä»¶ä¿¡æ¯
    if args.save_hardware_info:
        hardware_file = 'hardware_info.json'
        with open(hardware_file, 'w', encoding='utf-8') as f:
            json.dump(detector.hardware_info, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ ç¡¬ä»¶ä¿¡æ¯å·²ä¿å­˜: {hardware_file}")
    
    if args.detect_only:
        return
    
    # åŠ è½½å’Œä¼˜åŒ–é…ç½®
    try:
        original_config = load_config(args.config)
        print(f"\nğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
        
        optimizer = ConfigOptimizer(detector)
        optimized_config = optimizer.optimize_training_config(original_config)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = optimizer.generate_optimization_report(original_config, optimized_config)
        print(f"\n{report}")
        
        # ä¿å­˜ä¼˜åŒ–åçš„é…ç½®
        output_path = args.output or f"config_optimized_{detector.get_hardware_tier()}.yaml"
        save_config(optimized_config, output_path)
        print(f"\nğŸ’¾ ä¼˜åŒ–åé…ç½®å·²ä¿å­˜: {output_path}")
        
        print(f"\nğŸš€ ä½¿ç”¨ä¼˜åŒ–åçš„é…ç½®:")
        print(f"   python train.py --config {output_path}")
        
    except FileNotFoundError:
        print(f"\nâŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
    except Exception as e:
        print(f"\nâŒ é…ç½®ä¼˜åŒ–å¤±è´¥: {e}")


class DynamicConfigAdjuster:
    """åŠ¨æ€é…ç½®è°ƒæ•´å™¨"""

    def __init__(self, initial_config: Dict[str, Any]):
        self.config = initial_config.copy()
        self.performance_history = []
        self.adjustment_count = 0
        self.max_adjustments = 3

    def monitor_training_performance(self, epoch: int, batch_time: float,
                                   memory_usage: float, gpu_utilization: float = None) -> bool:
        """
        ç›‘æ§è®­ç»ƒæ€§èƒ½å¹¶å†³å®šæ˜¯å¦éœ€è¦è°ƒæ•´

        Args:
            epoch: å½“å‰è½®æ•°
            batch_time: æ‰¹æ¬¡å¤„ç†æ—¶é—´ï¼ˆç§’ï¼‰
            memory_usage: å†…å­˜ä½¿ç”¨ç‡ï¼ˆ0-1ï¼‰
            gpu_utilization: GPUä½¿ç”¨ç‡ï¼ˆ0-1ï¼‰

        Returns:
            æ˜¯å¦éœ€è¦è°ƒæ•´é…ç½®
        """
        performance = {
            'epoch': epoch,
            'batch_time': batch_time,
            'memory_usage': memory_usage,
            'gpu_utilization': gpu_utilization,
            'timestamp': time.time()
        }

        self.performance_history.append(performance)

        # ä¿ç•™æœ€è¿‘10ä¸ªè®°å½•
        if len(self.performance_history) > 10:
            self.performance_history.pop(0)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒæ•´
        if len(self.performance_history) >= 3 and self.adjustment_count < self.max_adjustments:
            return self._should_adjust()

        return False

    def _should_adjust(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è°ƒæ•´é…ç½®"""
        recent_performance = self.performance_history[-3:]

        # æ£€æŸ¥å†…å­˜ä½¿ç”¨ç‡
        avg_memory = sum(p['memory_usage'] for p in recent_performance) / len(recent_performance)
        if avg_memory > 0.9:  # å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡90%
            return True

        # æ£€æŸ¥æ‰¹æ¬¡å¤„ç†æ—¶é—´
        avg_batch_time = sum(p['batch_time'] for p in recent_performance) / len(recent_performance)
        if avg_batch_time > 10:  # æ‰¹æ¬¡å¤„ç†æ—¶é—´è¶…è¿‡10ç§’
            return True

        # æ£€æŸ¥GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœæœ‰ï¼‰
        gpu_utils = [p['gpu_utilization'] for p in recent_performance if p['gpu_utilization'] is not None]
        if gpu_utils:
            avg_gpu_util = sum(gpu_utils) / len(gpu_utils)
            if avg_gpu_util < 0.3:  # GPUä½¿ç”¨ç‡ä½äº30%
                return True

        return False

    def adjust_config(self) -> Dict[str, Any]:
        """è°ƒæ•´é…ç½®å‚æ•°"""
        if self.adjustment_count >= self.max_adjustments:
            return self.config

        recent_performance = self.performance_history[-3:]
        avg_memory = sum(p['memory_usage'] for p in recent_performance) / len(recent_performance)
        avg_batch_time = sum(p['batch_time'] for p in recent_performance) / len(recent_performance)

        adjustments = []

        # å†…å­˜å‹åŠ›å¤§ï¼Œå‡å°‘æ‰¹æ¬¡å¤§å°
        if avg_memory > 0.9:
            old_batch_size = self.config['data']['batch_size']
            new_batch_size = max(1, old_batch_size // 2)
            self.config['data']['batch_size'] = new_batch_size
            adjustments.append(f"æ‰¹æ¬¡å¤§å°: {old_batch_size} â†’ {new_batch_size} (å†…å­˜å‹åŠ›)")

        # å¤„ç†æ—¶é—´é•¿ï¼Œå‡å°‘æ•°æ®åŠ è½½çº¿ç¨‹
        if avg_batch_time > 10:
            old_workers = self.config['data']['num_workers']
            new_workers = max(1, old_workers - 2)
            self.config['data']['num_workers'] = new_workers
            adjustments.append(f"æ•°æ®çº¿ç¨‹: {old_workers} â†’ {new_workers} (å¤„ç†æ—¶é—´é•¿)")

        # GPUä½¿ç”¨ç‡ä½ï¼Œå¯èƒ½å¯ä»¥å¢åŠ æ‰¹æ¬¡å¤§å°
        gpu_utils = [p['gpu_utilization'] for p in recent_performance if p['gpu_utilization'] is not None]
        if gpu_utils and avg_memory < 0.7:
            avg_gpu_util = sum(gpu_utils) / len(gpu_utils)
            if avg_gpu_util < 0.3:
                old_batch_size = self.config['data']['batch_size']
                new_batch_size = min(64, old_batch_size + 4)
                self.config['data']['batch_size'] = new_batch_size
                adjustments.append(f"æ‰¹æ¬¡å¤§å°: {old_batch_size} â†’ {new_batch_size} (GPUåˆ©ç”¨ç‡ä½)")

        if adjustments:
            self.adjustment_count += 1
            print(f"\nğŸ”§ åŠ¨æ€è°ƒæ•´é…ç½® (ç¬¬{self.adjustment_count}æ¬¡):")
            for adj in adjustments:
                print(f"  {adj}")

        return self.config


import time


if __name__ == '__main__':
    main()
