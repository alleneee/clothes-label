#!/usr/bin/env python3
"""
é›†æˆHuggingFace Evaluateçš„è®­ç»ƒæ¨¡å—
ä¼˜åŒ–è¯„ä¼°æ–¹å¼ï¼Œæä¾›æ›´ä¸“ä¸šçš„æŒ‡æ ‡è®¡ç®—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
import numpy as np
from typing import Dict, List, Optional, Any, Tuple
import logging

from .hf_evaluate_module import HFEvaluateModule
from .train import ProductClassifier  # ç»§æ‰¿åŸæœ‰çš„è®­ç»ƒç±»

logger = logging.getLogger(__name__)


class ProductClassifierWithHFEvaluate(ProductClassifier):
    """é›†æˆHuggingFace Evaluateçš„å•†å“åˆ†ç±»å™¨"""
    
    def __init__(self, config, enable_dynamic_adjustment=False):
        super().__init__(config, enable_dynamic_adjustment)
        
        # åˆå§‹åŒ–HF Evaluateæ¨¡å—
        self.hf_evaluator = HFEvaluateModule(
            class_names=self.class_names,
            output_dir="evaluation_results",
            save_results=True
        )
        
        # å­˜å‚¨é¢„æµ‹ç»“æœç”¨äºè¯¦ç»†è¯„ä¼°
        self.validation_predictions = []
        self.validation_references = []
        self.validation_probabilities = []
        
        self.test_predictions = []
        self.test_references = []
        self.test_probabilities = []
        
        logger.info("âœ… å·²é›†æˆHuggingFace Evaluateè¯„ä¼°æ¨¡å—")
    
    def validation_step(self, batch, batch_idx):
        """éªŒè¯æ­¥éª¤ - é›†æˆHF Evaluate"""
        # å¤„ç†ä¸åŒçš„batchæ ¼å¼
        if isinstance(batch, dict):
            x, y = batch['image'], batch['label']
        else:
            x, y = batch
            
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # è®¡ç®—é¢„æµ‹ç»“æœå’Œæ¦‚ç‡
        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1)
        acc = torch.sum(preds == y).float() / len(y)
        
        # è®°å½•åŸºç¡€æŒ‡æ ‡
        self.log('val_loss', loss, on_epoch=True, prog_bar=True)
        self.log('val_acc', acc, on_epoch=True, prog_bar=True)
        
        # æ”¶é›†é¢„æµ‹ç»“æœç”¨äºè¯¦ç»†è¯„ä¼°
        self.validation_predictions.extend(preds.cpu().numpy())
        self.validation_references.extend(y.cpu().numpy())
        self.validation_probabilities.extend(probs.cpu().numpy())
        
        return {'val_loss': loss, 'val_acc': acc}
    
    def on_validation_epoch_end(self):
        """éªŒè¯è½®æ¬¡ç»“æŸæ—¶ä½¿ç”¨HF Evaluateè®¡ç®—è¯¦ç»†æŒ‡æ ‡"""
        if not self.validation_predictions:
            return
        
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            predictions = np.array(self.validation_predictions)
            references = np.array(self.validation_references)
            probabilities = np.array(self.validation_probabilities)
            
            # ä½¿ç”¨HF Evaluateè®¡ç®—æŒ‡æ ‡
            metrics = self.hf_evaluator.compute_metrics(
                predictions=predictions,
                references=references,
                probabilities=probabilities
            )
            
            # è®°å½•ä¸»è¦æŒ‡æ ‡åˆ°Lightning
            main_metrics = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']
            for metric_name in main_metrics:
                if metric_name in metrics:
                    self.log(f'val_{metric_name}', metrics[metric_name], on_epoch=True)
            
            # è®°å½•æ¯ä¸ªç±»åˆ«çš„F1å¾—åˆ†
            for class_name in self.class_names:
                f1_key = f'f1_{class_name}'
                if f1_key in metrics:
                    self.log(f'val_f1_{class_name}', metrics[f1_key], on_epoch=True)
            
            # è®°å½•èº«ä½“åˆ†ç±»ç‰¹æ®ŠæŒ‡æ ‡
            body_metrics = [k for k in metrics.keys() if 'body' in k and 'error_rate' in k]
            for metric_name in body_metrics:
                self.log(f'val_{metric_name}', metrics[metric_name], on_epoch=True)
                
        except Exception as e:
            logger.error(f"âŒ éªŒè¯æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        
        # æ¸…ç©ºç´¯ç§¯çš„é¢„æµ‹ç»“æœ
        self.validation_predictions.clear()
        self.validation_references.clear()
        self.validation_probabilities.clear()
    
    def test_step(self, batch, batch_idx):
        """æµ‹è¯•æ­¥éª¤ - é›†æˆHF Evaluate"""
        # å¤„ç†ä¸åŒçš„batchæ ¼å¼
        if isinstance(batch, dict):
            x, y = batch['image'], batch['label']
        else:
            x, y = batch
            
        logits = self(x)
        loss = self.criterion(logits, y)
        
        # è®¡ç®—é¢„æµ‹ç»“æœå’Œæ¦‚ç‡
        probs = F.softmax(logits, dim=1)
        preds = torch.argmax(logits, dim=1)
        acc = torch.sum(preds == y).float() / len(y)
        
        # è®°å½•åŸºç¡€æŒ‡æ ‡
        self.log('test_loss', loss, on_epoch=True)
        self.log('test_acc', acc, on_epoch=True)
        
        # æ”¶é›†é¢„æµ‹ç»“æœç”¨äºè¯¦ç»†è¯„ä¼°
        self.test_predictions.extend(preds.cpu().numpy())
        self.test_references.extend(y.cpu().numpy())
        self.test_probabilities.extend(probs.cpu().numpy())
        
        return {'test_loss': loss, 'test_acc': acc}
    
    def on_test_epoch_end(self):
        """æµ‹è¯•ç»“æŸæ—¶ä½¿ç”¨HF Evaluateç”Ÿæˆè¯¦ç»†æŠ¥å‘Š"""
        if not self.test_predictions:
            return
        
        try:
            # è½¬æ¢ä¸ºnumpyæ•°ç»„
            predictions = np.array(self.test_predictions)
            references = np.array(self.test_references)
            probabilities = np.array(self.test_probabilities)
            
            # ä½¿ç”¨HF Evaluateç”Ÿæˆå®Œæ•´æŠ¥å‘Š
            report = self.hf_evaluator.generate_evaluation_report(
                predictions=predictions,
                references=references,
                probabilities=probabilities,
                dataset_name="test"
            )
            
            # æ‰“å°è¯„ä¼°æ‘˜è¦
            self.hf_evaluator.print_evaluation_summary(report)
            
            # è®°å½•ä¸»è¦æŒ‡æ ‡åˆ°Lightning
            metrics = report['metrics']
            main_metrics = ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro', 'matthews_correlation']
            for metric_name in main_metrics:
                if metric_name in metrics:
                    self.log(f'test_{metric_name}', metrics[metric_name], on_epoch=True)
            
            # è®°å½•æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
            for class_name in self.class_names:
                for metric_type in ['f1', 'precision', 'recall', 'accuracy']:
                    metric_key = f'{metric_type}_{class_name}'
                    if metric_key in metrics:
                        self.log(f'test_{metric_key}', metrics[metric_key], on_epoch=True)
            
            # è®°å½•èº«ä½“åˆ†ç±»ç‰¹æ®ŠæŒ‡æ ‡
            body_metrics = [k for k in metrics.keys() if 'body' in k and 'error_rate' in k]
            for metric_name in body_metrics:
                self.log(f'test_{metric_name}', metrics[metric_name], on_epoch=True)
                
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•æŒ‡æ ‡è®¡ç®—å¤±è´¥: {e}")
        
        # æ¸…ç©ºç´¯ç§¯çš„é¢„æµ‹ç»“æœ
        self.test_predictions.clear()
        self.test_references.clear()
        self.test_probabilities.clear()
    
    def get_current_metrics(self, dataset_type: str = "validation") -> Dict[str, float]:
        """è·å–å½“å‰æŒ‡æ ‡ï¼ˆç”¨äºå¤–éƒ¨è°ƒç”¨ï¼‰"""
        if dataset_type == "validation":
            if not self.validation_predictions:
                return {}
            
            predictions = np.array(self.validation_predictions)
            references = np.array(self.validation_references)
            probabilities = np.array(self.validation_probabilities)
            
        elif dataset_type == "test":
            if not self.test_predictions:
                return {}
                
            predictions = np.array(self.test_predictions)
            references = np.array(self.test_references)
            probabilities = np.array(self.test_probabilities)
        else:
            return {}
        
        try:
            metrics = self.hf_evaluator.compute_metrics(
                predictions=predictions,
                references=references,
                probabilities=probabilities
            )
            return metrics
        except Exception as e:
            logger.error(f"âŒ è·å–å½“å‰æŒ‡æ ‡å¤±è´¥: {e}")
            return {}
    
    def evaluate_predictions(
        self,
        predictions: np.ndarray,
        references: np.ndarray,
        probabilities: Optional[np.ndarray] = None,
        dataset_name: str = "custom"
    ) -> Dict[str, Any]:
        """è¯„ä¼°è‡ªå®šä¹‰é¢„æµ‹ç»“æœ"""
        return self.hf_evaluator.generate_evaluation_report(
            predictions=predictions,
            references=references,
            probabilities=probabilities,
            dataset_name=dataset_name
        )


def create_hf_evaluate_classifier(config: Dict[str, Any]) -> ProductClassifierWithHFEvaluate:
    """åˆ›å»ºé›†æˆHF Evaluateçš„åˆ†ç±»å™¨"""
    return ProductClassifierWithHFEvaluate(config)


# å…¼å®¹æ€§å‡½æ•°
def get_classifier_class():
    """è·å–åˆ†ç±»å™¨ç±»"""
    return ProductClassifierWithHFEvaluate


if __name__ == "__main__":
    # æµ‹è¯•ç”¨æ³•
    import yaml
    
    # åŠ è½½é…ç½®
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = create_hf_evaluate_classifier(config)
    
    print("âœ… é›†æˆHF Evaluateçš„åˆ†ç±»å™¨åˆ›å»ºæˆåŠŸ")
    print(f"ğŸ“Š æ”¯æŒçš„è¯„ä¼°æŒ‡æ ‡: {classifier.hf_evaluator.metrics_to_compute}")
    print(f"ğŸ¯ ç±»åˆ«æ•°é‡: {len(classifier.class_names)}")
    print(f"ğŸ“ è¯„ä¼°ç»“æœä¿å­˜ç›®å½•: {classifier.hf_evaluator.output_dir}")
