#!/usr/bin/env python3
"""
HF Datasetsæ•°æ®æ¨¡å— - æ”¯æŒå¢é‡æ•°æ®æ‰©å±•
æ›¿ä»£åŸæœ‰å¤æ‚æ•°æ®å¤„ç†ï¼Œæä¾›ç®€æ´é«˜æ•ˆçš„æ•°æ®åŠ è½½æ–¹æ¡ˆ
"""

import os
import json
import hashlib
from pathlib import Path
from typing import Dict, List, Optional, Any, Union, Generator, Tuple
from datetime import datetime
import logging

import torch
import pytorch_lightning as pl
from torch.utils.data import DataLoader
from datasets import Dataset, DatasetDict, Features, Value, Image, load_dataset, concatenate_datasets
import datasets
from torchvision import transforms
import numpy as np
from PIL import Image as PILImage

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é™é»˜PILè­¦å‘Š
import warnings
warnings.filterwarnings('ignore', category=UserWarning, module='PIL')


class ClothesDatasetBuilder:
    """è¡£æœæ•°æ®é›†æ„å»ºå™¨ - æ”¯æŒå¢é‡æ•°æ®æ·»åŠ """
    
    def __init__(
        self,
        data_dir: str = "datasets/main/train/clothes",
        cache_dir: str = "datasets/hf_cache",
        dataset_name: str = "clothes_classification_v1",
        class_names: Optional[List[str]] = None,
        auto_detect: bool = True
    ):
        self.data_dir = Path(data_dir)
        self.cache_dir = Path(cache_dir)
        self.dataset_name = dataset_name
        self.auto_detect = auto_detect
        
        if class_names:
            self.class_names = list(class_names)
        elif auto_detect:
            self.class_names = self._discover_classes()
        else:
            raise ValueError("When auto_detect is False, class_names must be provided.")
        
        # åˆ›å»ºç±»åˆ«åˆ°IDçš„æ˜ å°„
        self.class_to_id = {name: idx for idx, name in enumerate(self.class_names)}
        self.id_to_class = {idx: name for idx, name in enumerate(self.class_names)}
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def _discover_classes(self) -> List[str]:
        """è‡ªåŠ¨å‘ç°æ•°æ®ç›®å½•ä¸­çš„ç±»åˆ«åç§°"""
        if not self.data_dir.exists():
            logger.warning(f"âš ï¸ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}, ä½¿ç”¨é»˜è®¤ç©ºç±»åˆ«åˆ—è¡¨")
            return []

        class_names = sorted([
            item.name for item in self.data_dir.iterdir() if item.is_dir()
        ])

        if not class_names:
            logger.warning(f"âš ï¸ åœ¨ {self.data_dir} ä¸­æœªå‘ç°ç±»åˆ«ç›®å½•")
        return class_names
        
    def _scan_images(self, scan_dir: Optional[Path] = None) -> Generator[Dict[str, Any], None, None]:
        """æ‰«æå›¾ç‰‡æ–‡ä»¶ï¼Œç”Ÿæˆæ•°æ®è®°å½•"""
        scan_dir = scan_dir or self.data_dir
        
        logger.info(f"ğŸ” æ‰«æå›¾ç‰‡ç›®å½•: {scan_dir}")
        
        supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
        
        for class_dir in scan_dir.iterdir():
            if not class_dir.is_dir():
                continue
                
            class_name = class_dir.name
            if class_name not in self.class_to_id:
                logger.warning(f"âš ï¸  æœªçŸ¥ç±»åˆ«: {class_name}")
                continue
                
            class_id = self.class_to_id[class_name]
            
            image_files = []
            for ext in supported_formats:
                image_files.extend(class_dir.glob(f"*{ext}"))
            
            logger.info(f"ğŸ“ {class_name}: å‘ç° {len(image_files)} å¼ å›¾ç‰‡")
            
            for img_path in image_files:
                try:
                    # éªŒè¯å›¾ç‰‡å¯ä»¥æ‰“å¼€ï¼Œå®½å®¹å¤„ç†æ ¼å¼ä¸åŒ¹é…çš„æ–‡ä»¶
                    with PILImage.open(img_path) as img:
                        width, height = img.size
                        # è®°å½•å®é™…æ ¼å¼
                        actual_format = img.format
                        
                    # ç”Ÿæˆå”¯ä¸€ID
                    relative_path = str(img_path.relative_to(scan_dir))
                    unique_id = hashlib.md5(relative_path.encode()).hexdigest()
                    
                    yield {
                        'id': unique_id,
                        'image': str(img_path),  # HF Datasetsä¼šè‡ªåŠ¨å¤„ç†å›¾ç‰‡åŠ è½½
                        'class_name': class_name,
                        'label': class_id,
                        'width': width,
                        'height': height,
                        'file_size': img_path.stat().st_size,
                        'relative_path': relative_path,
                        'scan_time': datetime.now().isoformat(),
                        'actual_format': actual_format,  # æ·»åŠ å®é™…æ ¼å¼ä¿¡æ¯
                    }
                    
                except Exception as e:
                    logger.warning(f"âš ï¸  æ— æ³•å¤„ç†å›¾ç‰‡ {img_path}: {e} - è·³è¿‡")
                    continue
    
    def build_dataset(self, force_rebuild: bool = False) -> Dataset:
        """æ„å»ºHF Dataset"""
        dataset_path = self.cache_dir / f"{self.dataset_name}.hf"
        
        # æ£€æŸ¥ç¼“å­˜
        if dataset_path.exists() and not force_rebuild:
            logger.info(f"ğŸ“¦ åŠ è½½ç¼“å­˜æ•°æ®é›†: {dataset_path}")
            try:
                return Dataset.load_from_disk(str(dataset_path))
            except Exception as e:
                logger.warning(f"âš ï¸  ç¼“å­˜åŠ è½½å¤±è´¥: {e}ï¼Œé‡æ–°æ„å»º")
        
        logger.info("ğŸ—ï¸  æ„å»ºæ–°æ•°æ®é›†...")
        
        # å®šä¹‰æ•°æ®é›†ç‰¹å¾
        features = Features({
            'id': Value('string'),
            'image': Image(),  # HF Datasetsçš„Imageç‰¹å¾
            'class_name': Value('string'),
            'label': datasets.ClassLabel(names=self.class_names),  # ä½¿ç”¨ClassLabelä»¥æ”¯æŒåˆ†å±‚é‡‡æ ·
            'width': Value('int32'),
            'height': Value('int32'),
            'file_size': Value('int64'),
            'relative_path': Value('string'),
            'scan_time': Value('string'),
        })
        
        # ä»ç”Ÿæˆå™¨åˆ›å»ºæ•°æ®é›†
        dataset = Dataset.from_generator(
            self._scan_images,
            features=features
        )
        
        # ä¿å­˜åˆ°ç¼“å­˜
        logger.info(f"ğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ°: {dataset_path}")
        dataset.save_to_disk(str(dataset_path))
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'dataset_name': self.dataset_name,
            'num_classes': len(self.class_names),
            'class_names': self.class_names,
            'class_to_id': self.class_to_id,
            'total_samples': len(dataset),
            'created_time': datetime.now().isoformat(),
            'data_dir': str(self.data_dir),
        }
        
        with open(self.cache_dir / f"{self.dataset_name}_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æ•°æ®é›†æ„å»ºå®Œæˆ: {len(dataset)} ä¸ªæ ·æœ¬")
        return dataset
    
    def add_new_data(self, new_data_dir: str) -> Dataset:
        """å¢é‡æ·»åŠ æ–°æ•°æ®"""
        logger.info(f"ğŸ“ˆ å¢é‡æ·»åŠ æ•°æ®: {new_data_dir}")
        
        # åŠ è½½ç°æœ‰æ•°æ®é›†
        existing_dataset = self.build_dataset(force_rebuild=False)
        existing_ids = set(existing_dataset['id'])
        
        # æ‰«ææ–°æ•°æ®
        new_data_path = Path(new_data_dir)
        new_records = []
        
        for record in self._scan_images(new_data_path):
            # ä¸ºæ–°æ•°æ®ç”Ÿæˆä¸åŒçš„IDå‰ç¼€ï¼Œç¡®ä¿ä¸é‡å¤
            record['id'] = f"new_{record['id']}"
            if record['id'] not in existing_ids:
                new_records.append(record)
        
        if not new_records:
            logger.info("ğŸ“­ æ²¡æœ‰å‘ç°æ–°æ•°æ®")
            return existing_dataset
        
        logger.info(f"ğŸ“¥ å‘ç° {len(new_records)} æ¡æ–°æ•°æ®")
        
        # åˆ›å»ºæ–°æ•°æ®çš„Dataset
        features = existing_dataset.features
        new_dataset = Dataset.from_list(new_records, features=features)
        
        # åˆå¹¶æ•°æ®é›†
        combined_dataset = concatenate_datasets([existing_dataset, new_dataset])
        
        # æ›´æ–°ç‰ˆæœ¬å·
        version_num = int(self.dataset_name.split('_v')[-1]) + 1
        new_dataset_name = f"clothes_classification_v{version_num}"
        
        # ä¿å­˜æ–°ç‰ˆæœ¬
        new_dataset_path = self.cache_dir / f"{new_dataset_name}.hf"
        combined_dataset.save_to_disk(str(new_dataset_path))
        
        # æ›´æ–°å…ƒæ•°æ®
        metadata = {
            'dataset_name': new_dataset_name,
            'previous_version': self.dataset_name,
            'num_classes': len(self.class_names),
            'class_names': self.class_names,
            'total_samples': len(combined_dataset),
            'new_samples': len(new_records),
            'updated_time': datetime.now().isoformat(),
        }
        
        with open(self.cache_dir / f"{new_dataset_name}_metadata.json", 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… æ•°æ®é›†æ›´æ–°å®Œæˆ: æ–°å¢ {len(new_records)} æ ·æœ¬ï¼Œæ€»è®¡ {len(combined_dataset)} æ ·æœ¬")
        
        # æ›´æ–°å½“å‰æ•°æ®é›†åç§°
        self.dataset_name = new_dataset_name
        
        return combined_dataset


class HFDatasetsModule(pl.LightningDataModule):
    """HF Datasetsæ•°æ®æ¨¡å— - PyTorch Lightningå…¼å®¹"""
    
    def __init__(
        self,
        data_dir: str = "datasets/main/train/clothes",
        cache_dir: str = "datasets/hf_cache", 
        dataset_name: str = "clothes_classification_v1",
        batch_size: int = 32,
        image_size: int = 384,
        num_workers: int = 8,
        train_split: float = 0.8,
        val_split: float = 0.2,
        pin_memory: bool = True,
        augmentation_enabled: bool = True,
        class_names: Optional[List[str]] = None,
        auto_detect_classes: bool = True,
        **kwargs
    ):
        super().__init__()
        
        # åŸºç¡€å‚æ•°
        self.data_dir = data_dir
        self.cache_dir = cache_dir
        self.dataset_name = dataset_name
        self.batch_size = batch_size
        self.image_size = image_size
        self.num_workers = num_workers
        self.train_split = train_split
        self.val_split = val_split
        self.pin_memory = pin_memory
        self.augmentation_enabled = augmentation_enabled
        
        # æ•°æ®é›†æ„å»ºå™¨
        self.builder = ClothesDatasetBuilder(
            data_dir=data_dir,
            cache_dir=cache_dir,
            dataset_name=dataset_name,
            class_names=class_names,
            auto_detect=auto_detect_classes
        )
        
        # æ•°æ®å˜æ¢
        self.setup_transforms()
        
        # æ•°æ®é›†å˜é‡
        self.dataset: Optional[Dataset] = None
        self.train_dataset: Optional[Dataset] = None
        self.val_dataset: Optional[Dataset] = None
        
        # ç±»åˆ«æƒé‡ï¼ˆå»¶è¿Ÿè®¡ç®—ï¼‰
        self._class_weights: Optional[torch.Tensor] = None
        
    def setup_transforms(self):
        """è®¾ç½®æ•°æ®å˜æ¢"""
        # è®­ç»ƒæ—¶çš„æ•°æ®å¢å¼º
        self.train_transform = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size)),
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomRotation(degrees=10),
            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ]) if self.augmentation_enabled else transforms.Compose([
            transforms.Resize((self.image_size, self.image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # éªŒè¯æ—¶çš„å˜æ¢ï¼ˆæ— å¢å¼ºï¼‰
        self.val_transform = transforms.Compose([
            transforms.Resize((self.image_size, self.image_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def setup(self, stage: str = None):
        """è®¾ç½®æ•°æ®é›†"""
        if self.dataset is None:
            # æ„å»ºæˆ–åŠ è½½æ•°æ®é›†
            self.dataset = self.builder.build_dataset()
        
        # åˆ†å‰²æ•°æ®é›†
        if stage == 'fit' or stage is None:
            # æŒ‰ç±»åˆ«åˆ†å±‚åˆ†å‰²ï¼Œä¿è¯æ¯ä¸ªç±»åˆ«çš„æ¯”ä¾‹
            split_dataset = self.dataset.train_test_split(
                test_size=self.val_split,
                seed=42,
                stratify_by_column='label'  # æŒ‰æ ‡ç­¾åˆ†å±‚
            )
            
            self.train_dataset = split_dataset['train']
            self.val_dataset = split_dataset['test']
            
            # è®¾ç½®æ•°æ®æ ¼å¼å’Œå˜æ¢
            self.train_dataset = self.train_dataset.with_transform(self._train_transform)
            self.val_dataset = self.val_dataset.with_transform(self._val_transform)
            
            logger.info(f"ğŸ“Š æ•°æ®é›†åˆ†å‰²å®Œæˆ:")
            logger.info(f"   è®­ç»ƒé›†: {len(self.train_dataset)} æ ·æœ¬")
            logger.info(f"   éªŒè¯é›†: {len(self.val_dataset)} æ ·æœ¬")
    
    def _train_transform(self, examples):
        """è®­ç»ƒæ•°æ®å˜æ¢å‡½æ•°"""
        images = []
        for img in examples['image']:
            # å¤„ç†è°ƒè‰²æ¿å›¾åƒå’Œé€æ˜åº¦é—®é¢˜
            if img.mode == 'P':
                img = img.convert('RGBA')
            img = img.convert('RGB')
            images.append(self.train_transform(img))
        return {
            'image': images,
            'label': examples['label']
        }
    
    def _val_transform(self, examples):
        """éªŒè¯æ•°æ®å˜æ¢å‡½æ•°"""
        images = []
        for img in examples['image']:
            # å¤„ç†è°ƒè‰²æ¿å›¾åƒå’Œé€æ˜åº¦é—®é¢˜
            if img.mode == 'P':
                img = img.convert('RGBA')
            img = img.convert('RGB')
            images.append(self.val_transform(img))
        return {
            'image': images,
            'label': examples['label']
        }
    
    def train_dataloader(self) -> DataLoader:
        """è®­ç»ƒæ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=self._collate_fn
        )
    
    def val_dataloader(self) -> DataLoader:
        """éªŒè¯æ•°æ®åŠ è½½å™¨"""
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            collate_fn=self._collate_fn
        )
    
    def test_dataloader(self) -> DataLoader:
        """æµ‹è¯•æ•°æ®åŠ è½½å™¨ï¼ˆä½¿ç”¨éªŒè¯é›†ï¼‰"""
        return self.val_dataloader()
    
    def _collate_fn(self, batch):
        """æ‰¹å¤„ç†å‡½æ•°"""
        # æ£€æŸ¥æ•°æ®ç±»å‹
        if isinstance(batch[0], dict):
            # æ­£å¸¸çš„dictç±»å‹
            images = torch.stack([item['image'] for item in batch])
            labels = torch.tensor([item['label'] for item in batch], dtype=torch.long)
        else:
            # å¦‚æœä¸æ˜¯dictï¼Œå°è¯•ç›´æ¥è§£æ
            images = torch.stack([item[0] if isinstance(item, (list, tuple)) else item for item in batch])
            labels = torch.tensor([item[1] if isinstance(item, (list, tuple)) else 0 for item in batch], dtype=torch.long)
        
        return {
            'image': images,
            'label': labels
        }
    
    def add_new_data(self, new_data_dir: str):
        """å¢é‡æ·»åŠ æ–°æ•°æ®"""
        logger.info(f"ğŸ“ˆ æ·»åŠ æ–°æ•°æ®åˆ°æ•°æ®æ¨¡å—: {new_data_dir}")
        
        # ä½¿ç”¨æ„å»ºå™¨æ·»åŠ æ–°æ•°æ®
        self.dataset = self.builder.add_new_data(new_data_dir)
        
        # é‡æ–°è®¾ç½®æ•°æ®é›†åˆ†å‰²
        self.setup()
        
        logger.info("âœ… æ–°æ•°æ®æ·»åŠ å®Œæˆï¼Œæ•°æ®é›†å·²æ›´æ–°")
    
    def get_class_distribution(self) -> Dict[str, int]:
        """è·å–ç±»åˆ«åˆ†å¸ƒ"""
        if self.dataset is None:
            self.dataset = self.builder.build_dataset()
        
        class_counts = {}
        for class_name in self.builder.class_names:
            class_id = self.builder.class_to_id[class_name]
            count = sum(1 for label in self.dataset['label'] if label == class_id)
            class_counts[class_name] = count
        
        return class_counts
    
    def get_class_weights(self) -> Optional[torch.Tensor]:
        """è®¡ç®—ç±»åˆ«æƒé‡ï¼Œç”¨äºè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜"""
        if self.dataset is None:
            self.dataset = self.builder.build_dataset()
        
        class_counts = list(self.get_class_distribution().values())
        total_samples = sum(class_counts)
        
        if total_samples == 0:
            return None
        
        # è®¡ç®—æƒé‡ï¼šç±»åˆ«æƒé‡ = æ€»æ ·æœ¬æ•° / (ç±»åˆ«æ•° * ç±»åˆ«æ ·æœ¬æ•°)
        class_weights = []
        for count in class_counts:
            if count > 0:
                weight = total_samples / (len(class_counts) * count)
                class_weights.append(weight)
            else:
                class_weights.append(0.0)
        
        return torch.tensor(class_weights, dtype=torch.float32)
    
    @property
    def class_weights(self) -> Optional[torch.Tensor]:
        """ç±»åˆ«æƒé‡å±æ€§"""
        if self._class_weights is None:
            self._class_weights = self.get_class_weights()
        return self._class_weights
    
    def get_dataset_info(self) -> Dict[str, Any]:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        if self.dataset is None:
            self.dataset = self.builder.build_dataset()
        
        return {
            'total_samples': len(self.dataset),
            'num_classes': len(self.builder.class_names),
            'class_names': self.builder.class_names,
            'class_distribution': self.get_class_distribution(),
            'dataset_name': self.dataset_name,
            'cache_dir': str(self.cache_dir),
        }


# å…¼å®¹æ€§å‡½æ•°ï¼Œæ–¹ä¾¿ä»ç°æœ‰ä»£ç è¿ç§»
def create_hf_datamodule(config: dict) -> HFDatasetsModule:
    """ä»é…ç½®åˆ›å»ºHF Datasetsæ•°æ®æ¨¡å—"""
    data_config = config.get('data', {})
    
    return HFDatasetsModule(
        data_dir=data_config.get('data_dir', 'datasets/main/train/clothes'),
        batch_size=data_config.get('batch_size', 32),
        image_size=data_config.get('image_size', 384),
        num_workers=data_config.get('num_workers', 8),
        augmentation_enabled=data_config.get('augmentation', {}).get('enabled', True),
    )


if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("ğŸš€ HF Datasetsæ¨¡å—æ¼”ç¤º")
    
    # åˆ›å»ºæ•°æ®æ¨¡å—
    dm = HFDatasetsModule()
    
    # æ„å»ºæ•°æ®é›†
    dm.setup()
    
    # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
    info = dm.get_dataset_info()
    print(f"ğŸ“Š æ•°æ®é›†ä¿¡æ¯: {info}")
    
    # æµ‹è¯•æ•°æ®åŠ è½½å™¨
    train_loader = dm.train_dataloader()
    batch = next(iter(train_loader))
    print(f"ğŸ“¦ æ‰¹æ¬¡å½¢çŠ¶: {batch['image'].shape}, {batch['label'].shape}") 